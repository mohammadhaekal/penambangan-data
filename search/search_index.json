{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Selamat Datang di Halaman Tugas Penambangan Data \u00b6 Profil Singkat \u00b6 Nama : Mohammad Haekal NIM : 180411100004 Kelas : Penambangan Data 5B Dosen Pengampu : Mulaab, S.Si., M.Kom Program Studi : Teknik Informatika","title":"Home"},{"location":"#selamat-datang-di-halaman-tugas-penambangan-data","text":"","title":"Selamat Datang di Halaman Tugas Penambangan Data"},{"location":"#profil-singkat","text":"Nama : Mohammad Haekal NIM : 180411100004 Kelas : Penambangan Data 5B Dosen Pengampu : Mulaab, S.Si., M.Kom Program Studi : Teknik Informatika","title":"Profil Singkat"},{"location":"fuzzy_c-means/","text":"Fuzzy C-Means Clustering \u00b6 Fuzzy C-Means Clustering adalah teknik clustering berdasarkan derajat keanggotaan terhadap pusat cluster. C pada C-Means Clustering adalah jumlah cluster yang dibuat. Algoritma \u00b6 1. Menyiapkan data yang akan digunakan \u00b6 from pandas import DataFrame import random import numpy as np from IPython.display import HTML , display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) Data = pandas . read_csv ( 'bank.csv' , sep = ';' ) Data = Data [[ 'bi_jangka_waktu' , 'jml_pinjaman' ]] . sample ( 6 , random_state = 42 ) D = Data . values print ( \"Table (D) >>\" ) table ( D ) 0 1 24 4000000 24 4500000 24 300000 12 4500000 24 3500000 18 3500000 n , m , c , w , T , e , P0 , t = * D . shape , 2 , 2 , 10 , 0.1 , 0 , 1 print ( \"Variables >>\" ) print ( \" n = %d \\n m = %d \\n c = %d \\n w = %d \\n T = %d \\n e = %f \\n P0 = %d \\n t = %d \" % ( n , m , c , w , T , e , P0 , t )) Hasilnya: Variables >> n = 6 m = 2 c = 2 w = 2 T = 10 e = 0.100000 P0 = 0 t = 1 2. Membuat matriks derajat cluster dengan ukuran (m x n) \u00b6 Membuat matriks dengan membangkitkan angka random. random . seed ( 42 ) U = np . array ([[ random . uniform ( 0 , 1 ) for _ in range ( c )] for _ in range ( n )]) print ( \"U >> \\n \" ) print ( U ) Hasilnya adalah sebagai berikut: U >> [[ 0.6394268 0.02501076 ] [ 0.27502932 0.22321074 ] [ 0.73647121 0.67669949 ] [ 0.89217957 0.08693883 ] [ 0.42192182 0.02979722 ] [ 0.21863797 0.50535529 ]] 3. Menghitung pusat cluster \u00b6 def cluster ( U , D , x , y ): return sum ([ U [ i , y ] ** w * D [ i , x ] for i in range ( n )]) / sum ([ U [ i , y ] ** w for i in range ( n )]) V = np . array ([[ cluster ( U , D , x , y ) for x in range ( m )] for y in range ( c )]) print ( \"V >> \\n \" ) print ( V ) Hasilnya: V >> [[ 1.91976299e+01 3.89286518e+06 ] [ 2.18982102e+01 3.27820938e+06 ]] 4. Hitung Fungsi Objektif pada t (Pt) \u00b6 def objective ( V , U , D ): return sum ([ sum ([ sum ([( D [ i , j ] - V [ k , j ]) ** 2 for j in range ( m )]) * ( U [ i , k ] ** w ) for k in range ( c )]) for i in range ( n )]) Pt = objective ( V , U , D ) print ( \"Pt >> \\n \" ) print ( Pt ) Pt >> 927269557333.7877 5. Hitung Ulang Matrik Derajat Kluster (U = c x n) \u00b6 def converge ( V , D , i , k ): return ( sum ([( D [ i , j ] - V [ k , j ]) ** 2 for j in range ( 2 )]) ** ( - 1 / ( w - 1 ))) / sum ([ sum ([( D [ i , j ] - V [ k , j ]) ** 2 for j in range ( 2 )]) ** ( - 1 / ( w - 1 )) for k in range ( 2 )]) print ( \"U >> \\n \" ) np . array ([[ converge ( V , D , i , k ) for k in range ( 2 )] for i in range ( 15 )]) U >> array ([[ 0.97844368 , 0.02155632 ], [ 0.80196859 , 0.19803141 ], [ 0.08849724 , 0.91150276 ], [ 0.80196859 , 0.19803141 ], [ 0.24168468 , 0.75831532 ], [ 0.24168468 , 0.75831532 ]]) 6. Cek Berhenti Atau Loop Kembali \u00b6 Mengecek apakah sudah berhenti atau harus loop kembali dengan menggunakan $$ P_t - P_{t-1} < e \\quad \\textrm{atau} \\quad t >= T $$ def iterate ( U ): V = np . array ([[ cluster ( U , D , x , y ) for x in range ( m )] for y in range ( c )]) return np . array ([[ converge ( V , D , i , k ) for k in range ( c )] for i in range ( n )]), objective ( V , U , D ) def fuzzyCM ( U ): #U = np.array([[random.uniform(0, 1) for _ in range(c)] for _ in range(n)]) U , P2 , P , t = * iterate ( U ), 0 , 1 while abs ( P2 - P ) > e and t < T : U , P2 , P , t = * iterate ( U ), P2 , t + 1 return U , t FuzzyResult , FuzzyIters = fuzzyCM ( U ) print ( \"Iterating %d times, fuzzy result >> \\n \" % FuzzyIters ) print ( FuzzyResult ) Iterating 10 times , fuzzy result >> [[ 0.72887053 0.27112947 ] [ 0.99094361 0.00905639 ] [ 0.06246895 0.93753105 ] [ 0.99094361 0.00905639 ] [ 0.02442136 0.97557864 ] [ 0.02442136 0.97557864 ]] 7. Ambil Nilai Terbesar pada kolom \u00b6 table ( DataFrame ([ D [ i ] . tolist () + [ np . argmax ( FuzzyResult [ i ] . tolist ())] for i in range ( 15 )], columns = Data . columns . tolist () + [ \"Cluster Index\" ])) bi_jangka_waktu jml_peminjaman Cluster Index 24 4000000 0 24 4500000 0 24 300000 1 12 4500000 0 24 3500000 1 18 3500000 1","title":"Fuzzy C-Means Cluster"},{"location":"fuzzy_c-means/#fuzzy-c-means-clustering","text":"Fuzzy C-Means Clustering adalah teknik clustering berdasarkan derajat keanggotaan terhadap pusat cluster. C pada C-Means Clustering adalah jumlah cluster yang dibuat.","title":"Fuzzy C-Means Clustering"},{"location":"fuzzy_c-means/#algoritma","text":"","title":"Algoritma"},{"location":"fuzzy_c-means/#1-menyiapkan-data-yang-akan-digunakan","text":"from pandas import DataFrame import random import numpy as np from IPython.display import HTML , display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) Data = pandas . read_csv ( 'bank.csv' , sep = ';' ) Data = Data [[ 'bi_jangka_waktu' , 'jml_pinjaman' ]] . sample ( 6 , random_state = 42 ) D = Data . values print ( \"Table (D) >>\" ) table ( D ) 0 1 24 4000000 24 4500000 24 300000 12 4500000 24 3500000 18 3500000 n , m , c , w , T , e , P0 , t = * D . shape , 2 , 2 , 10 , 0.1 , 0 , 1 print ( \"Variables >>\" ) print ( \" n = %d \\n m = %d \\n c = %d \\n w = %d \\n T = %d \\n e = %f \\n P0 = %d \\n t = %d \" % ( n , m , c , w , T , e , P0 , t )) Hasilnya: Variables >> n = 6 m = 2 c = 2 w = 2 T = 10 e = 0.100000 P0 = 0 t = 1","title":"1. Menyiapkan data yang akan digunakan"},{"location":"fuzzy_c-means/#2-membuat-matriks-derajat-cluster-dengan-ukuran-m-x-n","text":"Membuat matriks dengan membangkitkan angka random. random . seed ( 42 ) U = np . array ([[ random . uniform ( 0 , 1 ) for _ in range ( c )] for _ in range ( n )]) print ( \"U >> \\n \" ) print ( U ) Hasilnya adalah sebagai berikut: U >> [[ 0.6394268 0.02501076 ] [ 0.27502932 0.22321074 ] [ 0.73647121 0.67669949 ] [ 0.89217957 0.08693883 ] [ 0.42192182 0.02979722 ] [ 0.21863797 0.50535529 ]]","title":"2. Membuat matriks derajat cluster dengan ukuran (m x n)"},{"location":"fuzzy_c-means/#3-menghitung-pusat-cluster","text":"def cluster ( U , D , x , y ): return sum ([ U [ i , y ] ** w * D [ i , x ] for i in range ( n )]) / sum ([ U [ i , y ] ** w for i in range ( n )]) V = np . array ([[ cluster ( U , D , x , y ) for x in range ( m )] for y in range ( c )]) print ( \"V >> \\n \" ) print ( V ) Hasilnya: V >> [[ 1.91976299e+01 3.89286518e+06 ] [ 2.18982102e+01 3.27820938e+06 ]]","title":"3. Menghitung pusat cluster"},{"location":"fuzzy_c-means/#4-hitung-fungsi-objektif-pada-t-pt","text":"def objective ( V , U , D ): return sum ([ sum ([ sum ([( D [ i , j ] - V [ k , j ]) ** 2 for j in range ( m )]) * ( U [ i , k ] ** w ) for k in range ( c )]) for i in range ( n )]) Pt = objective ( V , U , D ) print ( \"Pt >> \\n \" ) print ( Pt ) Pt >> 927269557333.7877","title":"4. Hitung Fungsi Objektif pada t (Pt)"},{"location":"fuzzy_c-means/#5-hitung-ulang-matrik-derajat-kluster-u-c-x-n","text":"def converge ( V , D , i , k ): return ( sum ([( D [ i , j ] - V [ k , j ]) ** 2 for j in range ( 2 )]) ** ( - 1 / ( w - 1 ))) / sum ([ sum ([( D [ i , j ] - V [ k , j ]) ** 2 for j in range ( 2 )]) ** ( - 1 / ( w - 1 )) for k in range ( 2 )]) print ( \"U >> \\n \" ) np . array ([[ converge ( V , D , i , k ) for k in range ( 2 )] for i in range ( 15 )]) U >> array ([[ 0.97844368 , 0.02155632 ], [ 0.80196859 , 0.19803141 ], [ 0.08849724 , 0.91150276 ], [ 0.80196859 , 0.19803141 ], [ 0.24168468 , 0.75831532 ], [ 0.24168468 , 0.75831532 ]])","title":"5. Hitung Ulang Matrik Derajat Kluster (U = c x n)"},{"location":"fuzzy_c-means/#6-cek-berhenti-atau-loop-kembali","text":"Mengecek apakah sudah berhenti atau harus loop kembali dengan menggunakan $$ P_t - P_{t-1} < e \\quad \\textrm{atau} \\quad t >= T $$ def iterate ( U ): V = np . array ([[ cluster ( U , D , x , y ) for x in range ( m )] for y in range ( c )]) return np . array ([[ converge ( V , D , i , k ) for k in range ( c )] for i in range ( n )]), objective ( V , U , D ) def fuzzyCM ( U ): #U = np.array([[random.uniform(0, 1) for _ in range(c)] for _ in range(n)]) U , P2 , P , t = * iterate ( U ), 0 , 1 while abs ( P2 - P ) > e and t < T : U , P2 , P , t = * iterate ( U ), P2 , t + 1 return U , t FuzzyResult , FuzzyIters = fuzzyCM ( U ) print ( \"Iterating %d times, fuzzy result >> \\n \" % FuzzyIters ) print ( FuzzyResult ) Iterating 10 times , fuzzy result >> [[ 0.72887053 0.27112947 ] [ 0.99094361 0.00905639 ] [ 0.06246895 0.93753105 ] [ 0.99094361 0.00905639 ] [ 0.02442136 0.97557864 ] [ 0.02442136 0.97557864 ]]","title":"6. Cek Berhenti Atau Loop Kembali"},{"location":"fuzzy_c-means/#7-ambil-nilai-terbesar-pada-kolom","text":"table ( DataFrame ([ D [ i ] . tolist () + [ np . argmax ( FuzzyResult [ i ] . tolist ())] for i in range ( 15 )], columns = Data . columns . tolist () + [ \"Cluster Index\" ])) bi_jangka_waktu jml_peminjaman Cluster Index 24 4000000 0 24 4500000 0 24 300000 1 12 4500000 0 24 3500000 1 18 3500000 1","title":"7. Ambil Nilai Terbesar pada kolom"},{"location":"jarak_data/","text":"Menghitung Jarak Data \u00b6 Menghitung Jarak Data Numerik \u00b6 1. Minkowski Distance \u00b6 Keluarga Minkowski termasuk Manhattan distance dan Euclidean disrance. $$ \\begin{align} d_{min} = \\left(\\sum_{i=1}^{n}|x_i - y_i|^m\\right)^\\frac{1}{m}, m \\ge 1 \\end{align} $$ Dimana m m adalah bilangan positif dan x_i x_i dan y_i y_i adalah dua vektor yang berada di n -dimensi. Minkowski distance dapat bekerja dengan baik apabila kumpulan data terisolasi atau terpadatkan. Kelemahan dari Minkowski adalah data butuh dinormalisasikan agar bisa menggunakan metode ini. 2. Manhattan Distance \u00b6 Manhattan distance adalah kondisi khusus Minkowski dimana m m = 1. Manhattan distance sensitif terhadap data outliers. Jika ukuran ini digunakan dalam algoritma clustering, bentuk dari cluster itu adalah hyper-rectangular. $$ \\begin{align} d_{man} = \\sum_{i=1}^{n}|x_i - y_i| \\end{align} $$ 3. Euclidean Distance \u00b6 Euclidean distance adalah ukuran yang populer untuk data numerik. Kasus khusus yang menggunakan metode ini adalah Minkowski distance ketika m m = 2. Euclidean distance bekerja dengan baik ketika kumpulan data adalah kompak dan terisolasi. 4. Average Distance \u00b6 Average distance adalah versi modifikasi dari Euclidean untuk meningkatkan hasil. Average distace ada karena kekuarangan dari Euclidean distance. $$ \\begin{align} d_{ave} = (\\frac{1}{n}\\sum_{i=1}^{n}(x_i - y_i)^2)^\\frac{1}{2} \\end{align} $$ Dimana n adalah jumlah data. 5. Weighted Euclidean Distance \u00b6 Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. $$ \\begin{align} d_{we} = \\left(\\sum_{i=1}^{n}w_i(x_i - y_i)^2\\right)^\\frac{1}{2} \\end{align} $$ Dimana w_i w_i adalah bobot yang diberikan pada atribut ke i. 6. Chord Distance \u00b6 Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . $$ \\begin{align} d_{chord} = \\left(2-2\\frac{\\sum_{i=1}^{n}x_i y_i}{||x||_2||y||_2}\\right)^\\frac{1}{2} \\end{align} $$ Dimana: ||x||_2 = L^2 - norm ||x||_2 = \\sqrt{\\sum_{i=1}^{n}x_i^2} ||x||_2 = L^2 - norm ||x||_2 = \\sqrt{\\sum_{i=1}^{n}x_i^2} 7. Mahalanobis Distance \u00b6 Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. $$ \\begin{align} d_{mah} = \\sqrt{(x-y)S^{-1}(x-y)^T} \\end{align} $$ Dimana S S adalah matrik kovatian dari data. 8. Cosine Deasure \u00b6 Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen. $$ \\begin{align} Cosine(x,y) = \\frac{\\sum_{i=1}^{n}x_i y_i}{||x||_2||y||_2} \\end{align} $$ DImana ||y||_2 ||y||_2 adalah Euclidean norm dari vektor y = (y_1, y_2, ..., y_n y = (y_1, y_2, ..., y_n didefinisikan dengan ||y||_2 = \\sqrt{y_1^2+y_2^2+...+y_n^2} ||y||_2 = \\sqrt{y_1^2+y_2^2+...+y_n^2} 9. Pearson Correlation \u00b6 Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. $$ \\begin{align} Pearson(x,y)=\\frac{\\sum_{i=1}^{n}(x_i-\\mu_x)(y_i-\\mu_y)}{\\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}\\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}} \\end{align} $$ Kelemahan dari pearson correlation adalah sensitif terhadap data outlier. Menghitung Jarak Atribut Binary \u00b6 Similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Dissimilarity dan Similarity \u00b6 Rumus dissimilarity antara i i dan j j dan dinyatankan sebagai atribut biner simetris adalah: $$ \\begin{align} d(i,j) = \\frac {r+s}{q+r+s+t} \\end{align} $$ Rumus dissimilarity antara i i dan j j dan dinyatankan sebagai atribut biner asimetris adalah: $$ \\begin{align} d(i,j) = \\frac {r+s}{q+r+s} \\end{align} $$ Persamaan similarity Jaccard coefficient rumusnya adalah: $$ \\begin{align} sim(i,j) = \\frac {q}{q+r+s} = 1-d(i,j) \\end{align} $$ Mengukur Jarak Tipe Catergorical \u00b6 1. Overlay Metric \u00b6 Untuk semua atribut bertipe nominal, ukuran jarak yang paling sederhana adalah Overlay Metric (OM) dinyatakan dengan $$ \\begin{align} d(x,y) = \\sum_{i=1}^n\\delta(a_i(x),a_i(y)) \\end{align} $$ Dimana n n adalah banyaknya atribut, a_i(x) a_i(x) dan a_i(y) a_i(y) adalah nilai atribut ke i i yaitu A_i A_i dari masing-masing objek x x dan y y , \\delta(a_i(x))(a_i(y)) \\delta(a_i(x))(a_i(y)) adalah 0 jika a_i(x) a_i(x) = a_i(y) a_i(y) dan 1 jika sebaliknya. 2. Value Difference Metric \u00b6 VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan: $$ \\begin{align} d(x,y) = \\sum_{i=1}^n\\sum_{c=1}^C|P(c|a_i(x)) - P(c|a_i(y)) \\end{align} $$ Dimana C C adalah banyaknya kelas, P(c|a_i(x)) P(c|a_i(x)) adalah probabilitas bersyarat dimana kelas x x adalah c c dari atribut A_i A_i , yang memilki nilai a_i(x) a_i(x) , P(c|a_i(y)) P(c|a_i(y)) adalah probabilitas bersyarat dimana kelas y adalah c dengan atribut A_i A_i memiliki nilai a_i(y) a_i(y) 3. Minimum Risk Metric \u00b6 Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan: $$ \\begin{align} d(x,y) = \\sum_{c=1}^C|P(c|x) (1 - P(c|y)) \\end{align} $$ Mengukur Jarak Tipe Ordinal \u00b6 Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan f f adalah atribut-atribut dari atribut ordinak dan n n objek. Menghitung disimilarity terhadap f f fitur sebagai berikut: - Nilai f f untuk objek ke- i i adalah x_{if} x_{if} , dan f f memiliki M_f M_f status urutan, mewakili peringkat 1, ..., M_f 1, ..., M_f ganti setiap X_{if} X_{if} dengan peringkatnya r_{if} \\in {1...M_{M_f}} r_{if} \\in {1...M_{M_f}} - Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat r_{if} r_{if} dengan $$ \\begin{align} z_{if} = \\frac{r_{if} - 1}{M_f - 1} \\end{align} $$ - Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi z_{if} z_{if} Mengukur Jarak Tipe Campuran \u00b6 Tipe Campuran $$ \\begin{align} d(i,j) = \\frac{\\sum_{f=1}^p\\delta_{ij}^{(f)} d_{ij}^{(f)}}{\\sum_{f=1}^pd_{ij}^{(f)}} \\end{align} $$ Program \u00b6","title":"Menghitung Jarak Data"},{"location":"jarak_data/#menghitung-jarak-data","text":"","title":"Menghitung Jarak Data"},{"location":"jarak_data/#menghitung-jarak-data-numerik","text":"","title":"Menghitung Jarak Data Numerik"},{"location":"jarak_data/#1-minkowski-distance","text":"Keluarga Minkowski termasuk Manhattan distance dan Euclidean disrance. $$ \\begin{align} d_{min} = \\left(\\sum_{i=1}^{n}|x_i - y_i|^m\\right)^\\frac{1}{m}, m \\ge 1 \\end{align} $$ Dimana m m adalah bilangan positif dan x_i x_i dan y_i y_i adalah dua vektor yang berada di n -dimensi. Minkowski distance dapat bekerja dengan baik apabila kumpulan data terisolasi atau terpadatkan. Kelemahan dari Minkowski adalah data butuh dinormalisasikan agar bisa menggunakan metode ini.","title":"1. Minkowski Distance"},{"location":"jarak_data/#2-manhattan-distance","text":"Manhattan distance adalah kondisi khusus Minkowski dimana m m = 1. Manhattan distance sensitif terhadap data outliers. Jika ukuran ini digunakan dalam algoritma clustering, bentuk dari cluster itu adalah hyper-rectangular. $$ \\begin{align} d_{man} = \\sum_{i=1}^{n}|x_i - y_i| \\end{align} $$","title":"2. Manhattan Distance"},{"location":"jarak_data/#3-euclidean-distance","text":"Euclidean distance adalah ukuran yang populer untuk data numerik. Kasus khusus yang menggunakan metode ini adalah Minkowski distance ketika m m = 2. Euclidean distance bekerja dengan baik ketika kumpulan data adalah kompak dan terisolasi.","title":"3. Euclidean Distance"},{"location":"jarak_data/#4-average-distance","text":"Average distance adalah versi modifikasi dari Euclidean untuk meningkatkan hasil. Average distace ada karena kekuarangan dari Euclidean distance. $$ \\begin{align} d_{ave} = (\\frac{1}{n}\\sum_{i=1}^{n}(x_i - y_i)^2)^\\frac{1}{2} \\end{align} $$ Dimana n adalah jumlah data.","title":"4. Average Distance"},{"location":"jarak_data/#5-weighted-euclidean-distance","text":"Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. $$ \\begin{align} d_{we} = \\left(\\sum_{i=1}^{n}w_i(x_i - y_i)^2\\right)^\\frac{1}{2} \\end{align} $$ Dimana w_i w_i adalah bobot yang diberikan pada atribut ke i.","title":"5. Weighted Euclidean Distance"},{"location":"jarak_data/#6-chord-distance","text":"Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . $$ \\begin{align} d_{chord} = \\left(2-2\\frac{\\sum_{i=1}^{n}x_i y_i}{||x||_2||y||_2}\\right)^\\frac{1}{2} \\end{align} $$ Dimana: ||x||_2 = L^2 - norm ||x||_2 = \\sqrt{\\sum_{i=1}^{n}x_i^2} ||x||_2 = L^2 - norm ||x||_2 = \\sqrt{\\sum_{i=1}^{n}x_i^2}","title":"6. Chord Distance"},{"location":"jarak_data/#7-mahalanobis-distance","text":"Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. $$ \\begin{align} d_{mah} = \\sqrt{(x-y)S^{-1}(x-y)^T} \\end{align} $$ Dimana S S adalah matrik kovatian dari data.","title":"7. Mahalanobis Distance"},{"location":"jarak_data/#8-cosine-deasure","text":"Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen. $$ \\begin{align} Cosine(x,y) = \\frac{\\sum_{i=1}^{n}x_i y_i}{||x||_2||y||_2} \\end{align} $$ DImana ||y||_2 ||y||_2 adalah Euclidean norm dari vektor y = (y_1, y_2, ..., y_n y = (y_1, y_2, ..., y_n didefinisikan dengan ||y||_2 = \\sqrt{y_1^2+y_2^2+...+y_n^2} ||y||_2 = \\sqrt{y_1^2+y_2^2+...+y_n^2}","title":"8. Cosine Deasure"},{"location":"jarak_data/#9-pearson-correlation","text":"Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. $$ \\begin{align} Pearson(x,y)=\\frac{\\sum_{i=1}^{n}(x_i-\\mu_x)(y_i-\\mu_y)}{\\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}\\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}} \\end{align} $$ Kelemahan dari pearson correlation adalah sensitif terhadap data outlier.","title":"9. Pearson Correlation"},{"location":"jarak_data/#menghitung-jarak-atribut-binary","text":"Similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi.","title":"Menghitung Jarak Atribut Binary"},{"location":"jarak_data/#dissimilarity-dan-similarity","text":"Rumus dissimilarity antara i i dan j j dan dinyatankan sebagai atribut biner simetris adalah: $$ \\begin{align} d(i,j) = \\frac {r+s}{q+r+s+t} \\end{align} $$ Rumus dissimilarity antara i i dan j j dan dinyatankan sebagai atribut biner asimetris adalah: $$ \\begin{align} d(i,j) = \\frac {r+s}{q+r+s} \\end{align} $$ Persamaan similarity Jaccard coefficient rumusnya adalah: $$ \\begin{align} sim(i,j) = \\frac {q}{q+r+s} = 1-d(i,j) \\end{align} $$","title":"Dissimilarity dan Similarity"},{"location":"jarak_data/#mengukur-jarak-tipe-catergorical","text":"","title":"Mengukur Jarak Tipe Catergorical"},{"location":"jarak_data/#1-overlay-metric","text":"Untuk semua atribut bertipe nominal, ukuran jarak yang paling sederhana adalah Overlay Metric (OM) dinyatakan dengan $$ \\begin{align} d(x,y) = \\sum_{i=1}^n\\delta(a_i(x),a_i(y)) \\end{align} $$ Dimana n n adalah banyaknya atribut, a_i(x) a_i(x) dan a_i(y) a_i(y) adalah nilai atribut ke i i yaitu A_i A_i dari masing-masing objek x x dan y y , \\delta(a_i(x))(a_i(y)) \\delta(a_i(x))(a_i(y)) adalah 0 jika a_i(x) a_i(x) = a_i(y) a_i(y) dan 1 jika sebaliknya.","title":"1. Overlay Metric"},{"location":"jarak_data/#2-value-difference-metric","text":"VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan: $$ \\begin{align} d(x,y) = \\sum_{i=1}^n\\sum_{c=1}^C|P(c|a_i(x)) - P(c|a_i(y)) \\end{align} $$ Dimana C C adalah banyaknya kelas, P(c|a_i(x)) P(c|a_i(x)) adalah probabilitas bersyarat dimana kelas x x adalah c c dari atribut A_i A_i , yang memilki nilai a_i(x) a_i(x) , P(c|a_i(y)) P(c|a_i(y)) adalah probabilitas bersyarat dimana kelas y adalah c dengan atribut A_i A_i memiliki nilai a_i(y) a_i(y)","title":"2. Value Difference Metric"},{"location":"jarak_data/#3-minimum-risk-metric","text":"Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan: $$ \\begin{align} d(x,y) = \\sum_{c=1}^C|P(c|x) (1 - P(c|y)) \\end{align} $$","title":"3. Minimum Risk Metric"},{"location":"jarak_data/#mengukur-jarak-tipe-ordinal","text":"Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan f f adalah atribut-atribut dari atribut ordinak dan n n objek. Menghitung disimilarity terhadap f f fitur sebagai berikut: - Nilai f f untuk objek ke- i i adalah x_{if} x_{if} , dan f f memiliki M_f M_f status urutan, mewakili peringkat 1, ..., M_f 1, ..., M_f ganti setiap X_{if} X_{if} dengan peringkatnya r_{if} \\in {1...M_{M_f}} r_{if} \\in {1...M_{M_f}} - Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat r_{if} r_{if} dengan $$ \\begin{align} z_{if} = \\frac{r_{if} - 1}{M_f - 1} \\end{align} $$ - Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi z_{if} z_{if}","title":"Mengukur Jarak Tipe Ordinal"},{"location":"jarak_data/#mengukur-jarak-tipe-campuran","text":"Tipe Campuran $$ \\begin{align} d(i,j) = \\frac{\\sum_{f=1}^p\\delta_{ij}^{(f)} d_{ij}^{(f)}}{\\sum_{f=1}^pd_{ij}^{(f)}} \\end{align} $$","title":"Mengukur Jarak Tipe Campuran"},{"location":"jarak_data/#program","text":"","title":"Program"},{"location":"naive_bayes_classifier/","text":"Naive Bayes Classifier \u00b6 Naive Bayes Classifier adalah salah satu metode untuk mencari sebuah class dari sekumpultan fitur menggunakan probabilitas. Data yang digunakan adalah Data Bunga Iris \u00b6 from sklearn import datasets from pandas import * from numpy import * from math import * from IPython.display import HTML , display ; from tabulate import tabulate def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) iris = datasets . load_iris () data = [ list ( s ) + [ iris . target_names [ iris . target [ i ]]] for i , s in enumerate ( iris . data )] dataset = DataFrame ( data , columns = iris . feature_names + [ 'class' ]) . sample ( frac = 0.2 ) table ( dataset ) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.2 3.5 1.5 0.2 setosa 5 2.3 3.3 1 versicolor 7.7 2.6 6.9 2.3 virginica 5.1 3.7 1.5 0.4 setosa 5.2 4.1 1.5 0.1 setosa 4.3 3 1.1 0.1 setosa 4.9 2.4 3.3 1 versicolor 5.5 2.4 3.7 1 versicolor 6.7 3.1 4.7 1.5 versicolor 5 3.5 1.6 0.6 setosa 6.1 3 4.9 1.8 virginica 5 3.5 1.3 0.3 setosa 6.9 3.2 5.7 2.3 virginica 4.9 3 1.4 0.2 setosa 7.2 3.2 6 1.8 virginica 5.4 3 4.5 1.5 versicolor 5.6 2.7 4.2 1.3 versicolor 4.6 3.2 1.4 0.2 setosa 5.8 2.6 4 1.2 versicolor 4.8 3 1.4 0.3 setosa 4.8 3 1.4 0.1 setosa 7.6 3 6.6 2.1 virginica 5.7 4.4 1.5 0.4 setosa 5.6 2.9 3.6 1.3 versicolor 6.7 3.1 4.4 1.4 versicolor 7.7 3 6.1 2.3 virginica 5.7 2.9 4.2 1.3 versicolor 6.7 3 5 1.7 versicolor 4.7 3.2 1.6 0.2 setosa 5.4 3.4 1.7 0.2 setosa Mengecek Rata-Rata dan Sigma untuk Seluruh Class \u00b6 dataset_classes = {} for key , group in dataset . groupby ( 'class' ): mu_s = [ group [ c ] . mean () for c in group . columns [: - 1 ]] sigma_s = [ group [ c ] . std () for c in group . columns [: - 1 ]] dataset_classes [ key ] = [ group , mu_s , sigma_s ] print ( key , \"===>\" ) print ( 'Mu_s =>' , array ( mu_s )) print ( 'Sigma_s =>' , array ( sigma_s )) table ( group ) Untuk class Setosa \u00b6 setosa ===> Mu_s => [5.18333333 3.56666667 1.51666667 0.31666667] Sigma_s => [0.3250641 0.22509257 0.14719601 0.1602082 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5 3.5 1.6 0.6 setosa 5.2 3.4 1.4 0.2 setosa 5 3.4 1.5 0.2 setosa 5.4 3.9 1.3 0.4 setosa 4.8 3.4 1.6 0.2 setosa 5.7 3.8 1.7 0.3 setosa Untuk class Versicolor \u00b6 versicolor ===> Mu_s => [5.89166667 2.76666667 4.13333333 1.26666667] Sigma_s => [0.52476546 0.33393884 0.46188022 0.21461735] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.7 2.8 4.1 1.3 versicolor 5.5 2.6 4.4 1.2 versicolor 5.5 2.4 3.7 1 versicolor 6 3.4 4.5 1.6 versicolor 5.7 2.6 3.5 1 versicolor 6 2.9 4.5 1.5 versicolor 5.6 2.5 3.9 1.1 versicolor 6.8 2.8 4.8 1.4 versicolor 6.7 3.1 4.4 1.4 versicolor 5.8 2.6 4 1.2 versicolor 6.4 3.2 4.5 1.5 versicolor 5 2.3 3.3 1 versicolor Untuk class Virginica \u00b6 virginica ===> Mu_s => [6.61666667 3.13333333 5.58333333 2.06666667] Sigma_s => [0.7790826 0.34465617 0.59670814 0.23868326] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.3 3.3 6 2.5 virginica 6.4 3.1 5.5 1.8 virginica 5.8 2.7 5.1 1.9 virginica 6.7 3.1 5.6 2.4 virginica 6.5 3 5.2 2 virginica 5.6 2.8 4.9 2 virginica 5.9 3 5.1 1.8 virginica 7.7 3 6.1 2.3 virginica 6.8 3 5.5 2.1 virginica 7.7 3.8 6.7 2.2 virginica 6.1 3 4.9 1.8 virginica 7.9 3.8 6.4 2 virginica Menghitung Prior dan Likehood \u00b6 def numericalPriorProbability ( v , mu , sigma ): return ( 1.0 / sqrt ( 2 * pi * ( sigma ** 2 )) * exp ( - (( v - mu ) ** 2 ) / ( 2 * ( sigma ** 2 )))) def categoricalProbability ( sample , universe ): return sample . shape [ 0 ] / universe . shape [ 0 ] Ps = ([[ y ] + [ numericalPriorProbability ( x , d [ 1 ][ i ], d [ 2 ][ i ]) for i , x in enumerate ( test )] + [ categoricalProbability ( d [ 0 ], dataset )] for y , d in dataset_classes . items ()]) table ( DataFrame ( Ps , columns = [ \"classes\" ] + [ \"P( %d | C )\" % d for d in test ] + [ \"P( C )\" ])) classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 1.96232e-10 2.77721e-09 0.0123515 4.13093e-115 0.2 versicolor 1.93812e-07 2.31644e-10 2.01329e-05 1.11579e-35 0.4 virginica 1.07096e-05 4.94165e-07 9.87125e-09 9.46396e-15 0.4 Ranking \u00b6 Pss = ([[ r [ 0 ], prod ( r [ 1 :])] for r in Ps ]) PDss = DataFrame ( Pss , columns = [ 'class' , 'probability' ]) . sort_values ( 'probability' )[:: - 1 ] table ( PDss ) class probability virginica 1.97766e-34 versicolor 4.03412e-57 setosa 5.56132e-136 Mengetes Kebenaran Data \u00b6 def predict ( sampel ): priorLikehoods = ([[ y ] + [ numericalPriorProbability ( x , d [ 1 ][ i ], d [ 2 ][ i ]) for i , x in enumerate ( sampel )] + [ categoricalProbability ( d [ 0 ], dataset )] for y , d in dataset_classes . items ()]) products = ([[ r [ 0 ], prod ( r [ 1 :])] for r in priorLikehoods ]) result = DataFrame ( products , columns = [ 'class' , 'probability' ]) . sort_values ( 'probability' )[:: - 1 ] return result . values [ 0 , 0 ] dataset_test = DataFrame ([ list ( d ) + [ predict ( d [: 4 ])] for d in data ], columns = list ( dataset . columns ) + [ 'predicted class (by predict())' ]) table ( dataset_test ) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor versicolor 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor versicolor 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor virginica 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor versicolor 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor versicolor 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica virginica 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica virginica 6.1 3 4.9 1.8 virginica virginica 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica versicolor 6.1 2.6 5.6 1.4 virginica versicolor 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica virginica 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica Kesimpulan Data \u00b6 corrects = dataset_test . loc [ dataset_test [ 'class' ] == dataset_test [ 'predicted class (by predict())' ]] . shape [ 0 ] print ( 'Prediksi Training Bayes: %d of %d == %f %% ' % ( corrects , len ( data ), corrects / len ( data ) * 100 )) Prediksi Training Bayes: 144 of 150 == 96.000000 %","title":"Naive Bayes Classifier"},{"location":"naive_bayes_classifier/#naive-bayes-classifier","text":"Naive Bayes Classifier adalah salah satu metode untuk mencari sebuah class dari sekumpultan fitur menggunakan probabilitas.","title":"Naive Bayes Classifier"},{"location":"naive_bayes_classifier/#data-yang-digunakan-adalah-data-bunga-iris","text":"from sklearn import datasets from pandas import * from numpy import * from math import * from IPython.display import HTML , display ; from tabulate import tabulate def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) iris = datasets . load_iris () data = [ list ( s ) + [ iris . target_names [ iris . target [ i ]]] for i , s in enumerate ( iris . data )] dataset = DataFrame ( data , columns = iris . feature_names + [ 'class' ]) . sample ( frac = 0.2 ) table ( dataset ) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.2 3.5 1.5 0.2 setosa 5 2.3 3.3 1 versicolor 7.7 2.6 6.9 2.3 virginica 5.1 3.7 1.5 0.4 setosa 5.2 4.1 1.5 0.1 setosa 4.3 3 1.1 0.1 setosa 4.9 2.4 3.3 1 versicolor 5.5 2.4 3.7 1 versicolor 6.7 3.1 4.7 1.5 versicolor 5 3.5 1.6 0.6 setosa 6.1 3 4.9 1.8 virginica 5 3.5 1.3 0.3 setosa 6.9 3.2 5.7 2.3 virginica 4.9 3 1.4 0.2 setosa 7.2 3.2 6 1.8 virginica 5.4 3 4.5 1.5 versicolor 5.6 2.7 4.2 1.3 versicolor 4.6 3.2 1.4 0.2 setosa 5.8 2.6 4 1.2 versicolor 4.8 3 1.4 0.3 setosa 4.8 3 1.4 0.1 setosa 7.6 3 6.6 2.1 virginica 5.7 4.4 1.5 0.4 setosa 5.6 2.9 3.6 1.3 versicolor 6.7 3.1 4.4 1.4 versicolor 7.7 3 6.1 2.3 virginica 5.7 2.9 4.2 1.3 versicolor 6.7 3 5 1.7 versicolor 4.7 3.2 1.6 0.2 setosa 5.4 3.4 1.7 0.2 setosa","title":"Data yang digunakan adalah Data Bunga Iris"},{"location":"naive_bayes_classifier/#mengecek-rata-rata-dan-sigma-untuk-seluruh-class","text":"dataset_classes = {} for key , group in dataset . groupby ( 'class' ): mu_s = [ group [ c ] . mean () for c in group . columns [: - 1 ]] sigma_s = [ group [ c ] . std () for c in group . columns [: - 1 ]] dataset_classes [ key ] = [ group , mu_s , sigma_s ] print ( key , \"===>\" ) print ( 'Mu_s =>' , array ( mu_s )) print ( 'Sigma_s =>' , array ( sigma_s )) table ( group )","title":"Mengecek Rata-Rata dan Sigma untuk Seluruh Class"},{"location":"naive_bayes_classifier/#untuk-class-setosa","text":"setosa ===> Mu_s => [5.18333333 3.56666667 1.51666667 0.31666667] Sigma_s => [0.3250641 0.22509257 0.14719601 0.1602082 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5 3.5 1.6 0.6 setosa 5.2 3.4 1.4 0.2 setosa 5 3.4 1.5 0.2 setosa 5.4 3.9 1.3 0.4 setosa 4.8 3.4 1.6 0.2 setosa 5.7 3.8 1.7 0.3 setosa","title":"Untuk class Setosa"},{"location":"naive_bayes_classifier/#untuk-class-versicolor","text":"versicolor ===> Mu_s => [5.89166667 2.76666667 4.13333333 1.26666667] Sigma_s => [0.52476546 0.33393884 0.46188022 0.21461735] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.7 2.8 4.1 1.3 versicolor 5.5 2.6 4.4 1.2 versicolor 5.5 2.4 3.7 1 versicolor 6 3.4 4.5 1.6 versicolor 5.7 2.6 3.5 1 versicolor 6 2.9 4.5 1.5 versicolor 5.6 2.5 3.9 1.1 versicolor 6.8 2.8 4.8 1.4 versicolor 6.7 3.1 4.4 1.4 versicolor 5.8 2.6 4 1.2 versicolor 6.4 3.2 4.5 1.5 versicolor 5 2.3 3.3 1 versicolor","title":"Untuk class Versicolor"},{"location":"naive_bayes_classifier/#untuk-class-virginica","text":"virginica ===> Mu_s => [6.61666667 3.13333333 5.58333333 2.06666667] Sigma_s => [0.7790826 0.34465617 0.59670814 0.23868326] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.3 3.3 6 2.5 virginica 6.4 3.1 5.5 1.8 virginica 5.8 2.7 5.1 1.9 virginica 6.7 3.1 5.6 2.4 virginica 6.5 3 5.2 2 virginica 5.6 2.8 4.9 2 virginica 5.9 3 5.1 1.8 virginica 7.7 3 6.1 2.3 virginica 6.8 3 5.5 2.1 virginica 7.7 3.8 6.7 2.2 virginica 6.1 3 4.9 1.8 virginica 7.9 3.8 6.4 2 virginica","title":"Untuk class Virginica"},{"location":"naive_bayes_classifier/#menghitung-prior-dan-likehood","text":"def numericalPriorProbability ( v , mu , sigma ): return ( 1.0 / sqrt ( 2 * pi * ( sigma ** 2 )) * exp ( - (( v - mu ) ** 2 ) / ( 2 * ( sigma ** 2 )))) def categoricalProbability ( sample , universe ): return sample . shape [ 0 ] / universe . shape [ 0 ] Ps = ([[ y ] + [ numericalPriorProbability ( x , d [ 1 ][ i ], d [ 2 ][ i ]) for i , x in enumerate ( test )] + [ categoricalProbability ( d [ 0 ], dataset )] for y , d in dataset_classes . items ()]) table ( DataFrame ( Ps , columns = [ \"classes\" ] + [ \"P( %d | C )\" % d for d in test ] + [ \"P( C )\" ])) classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 1.96232e-10 2.77721e-09 0.0123515 4.13093e-115 0.2 versicolor 1.93812e-07 2.31644e-10 2.01329e-05 1.11579e-35 0.4 virginica 1.07096e-05 4.94165e-07 9.87125e-09 9.46396e-15 0.4","title":"Menghitung Prior dan Likehood"},{"location":"naive_bayes_classifier/#ranking","text":"Pss = ([[ r [ 0 ], prod ( r [ 1 :])] for r in Ps ]) PDss = DataFrame ( Pss , columns = [ 'class' , 'probability' ]) . sort_values ( 'probability' )[:: - 1 ] table ( PDss ) class probability virginica 1.97766e-34 versicolor 4.03412e-57 setosa 5.56132e-136","title":"Ranking"},{"location":"naive_bayes_classifier/#mengetes-kebenaran-data","text":"def predict ( sampel ): priorLikehoods = ([[ y ] + [ numericalPriorProbability ( x , d [ 1 ][ i ], d [ 2 ][ i ]) for i , x in enumerate ( sampel )] + [ categoricalProbability ( d [ 0 ], dataset )] for y , d in dataset_classes . items ()]) products = ([[ r [ 0 ], prod ( r [ 1 :])] for r in priorLikehoods ]) result = DataFrame ( products , columns = [ 'class' , 'probability' ]) . sort_values ( 'probability' )[:: - 1 ] return result . values [ 0 , 0 ] dataset_test = DataFrame ([ list ( d ) + [ predict ( d [: 4 ])] for d in data ], columns = list ( dataset . columns ) + [ 'predicted class (by predict())' ]) table ( dataset_test ) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor versicolor 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor versicolor 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor virginica 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor versicolor 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor versicolor 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica virginica 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica virginica 6.1 3 4.9 1.8 virginica virginica 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica versicolor 6.1 2.6 5.6 1.4 virginica versicolor 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica virginica 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica","title":"Mengetes Kebenaran Data"},{"location":"naive_bayes_classifier/#kesimpulan-data","text":"corrects = dataset_test . loc [ dataset_test [ 'class' ] == dataset_test [ 'predicted class (by predict())' ]] . shape [ 0 ] print ( 'Prediksi Training Bayes: %d of %d == %f %% ' % ( corrects , len ( data ), corrects / len ( data ) * 100 )) Prediksi Training Bayes: 144 of 150 == 96.000000 %","title":"Kesimpulan Data"},{"location":"seleksi_fitur/","text":"Seleksi Fitur \u00b6 Seleksi Fitur digunakan untuk mempermudah dalam mencari fitur data yang dibutuhkan atau fitur data yang penting dengan mengurangi jumlah fitur. Dengan cara ini, kita tidak kebingungan untuk menentukan fitur yang penting karena banyaknya fitur dalam data. from pandas import * from IPython.display import HTML , display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) Contoh Data \u00b6 Data ini yang digunakan untuk melakukan seleksi fitur. Teknik yang digunakan adalah Information Gain. df = read_csv ( 'data.csv' , sep = ';' ) table ( df ) Outlook Temperature Humidity Windy Play Sunny Hot High False No Sunny Hot High True No Overcast Hot High False Yes Rainy Mild High False Yes Rainy Cool Normal False Yes Rainy Cool Normal True No Overcast Cool Normal True Yes Sunny Mild High False No Sunny Cool Normal False Yes Rainy Mild Normal False Yes Sunny Mild Normal True Yes Overcast Mild High True Yes Overcast Hot Normal False Yes Rainy Mild High True No Entropy \u00b6 Rumus mencari entropy adalah: $$ E(T)= \\sum_{i=1}^c - P_i log_2 P_i $$ def findEntropy ( column ): rawGroups = df . groupby ( column ) targetGroups = [[ key , len ( data ), len ( data ) / df [ column ] . size ] for key , data in rawGroups ] targetGroups = DataFrame ( targetGroups , columns = [ 'value' , 'count' , 'probability' ]) return sum ([ - x * log ( x , 2 ) for x in targetGroups [ 'probability' ]]), targetGroups , rawGroups entropyTarget , groupTargets , _ = findEntropy ( 'play' ) table ( groupTargets ) print ( 'entropy target =' , entropyTarget ) Value Count Probability No 5 0.357 Yes 9 0.643 Entropy target = 0.940 Gain \u00b6 Information gain adalah salah satu metode untuk mendapatkan atribut-atribut yang paling berpengaruh terhadap dataset. Rumus dari Information Gain adalah: $$ Gain(T,X) = Entropy(T)-\\sum_{v \\in T}\\frac{T_{x,v}}{T} E(T_{x,v}) $$ def findGain ( column ): entropyOutlook , groupOutlooks , rawOutlooks = findEntropy ( column ) table ( groupOutlooks ) gain = entropyTarget - sum ( len ( data ) / len ( df ) * sum ( - x / len ( data ) * log ( x / len ( data ), 2 ) for x in data . groupby ( 'play' ) . size ()) for key , data in rawOutlooks ) print ( \"gain of\" , column , \"is\" , gain ) return gain gains = [[ x , findGain ( x )] for x in [ 'outlook' , 'temperature' , 'humidity' , 'windy' ]] Value Count Probability Overcast 4 0.286 Rainy 5 0.357 Sunny 5 0.357 Gain of Outlook is 0.247 Value Count Probability Cool 4 0.286 Hot 4 0.286 Mild 6 0.429 Gain of Temperature is 0.029 Value Count Probability High 7 0.5 Normal 7 0.5 Gain of Humidity is 0.152 Value Count Probability False 8 0.571 True 6 0.429 Gain of Windy is 0.048 Overall Gain Rank \u00b6 table ( DataFrame ( gains , columns = [ \"Feature\" , \"Gain Score\" ]) . sort_values ( \"Gain Score\" )[:: - 1 ]) Feature Gain Score Outlook 0.247 Humidity 0.152 Windy 0.048 Temperature 0.029 Hasil Akhir \u00b6 Dari seleksi fitur ini, setelah diurutkan berdasarkan nilai gain maka fitur yang tidak diperlukan lagi adalah Windy dan Temperature dan fitur yang digunakan hanya Outlook dan Humidity. Outlook Humidity Play Sunny High No Sunny High No Overcast High Yes Rainy High Yes Rainy Normal Yes Rainy Normal No Overcast Normal Yes Sunny High No Sunny Normal Yes Rainy Normal Yes Sunny Normal Yes Overcast High Yes Overcast Normal Yes Rainy High No","title":"Seleksi Fitur"},{"location":"seleksi_fitur/#seleksi-fitur","text":"Seleksi Fitur digunakan untuk mempermudah dalam mencari fitur data yang dibutuhkan atau fitur data yang penting dengan mengurangi jumlah fitur. Dengan cara ini, kita tidak kebingungan untuk menentukan fitur yang penting karena banyaknya fitur dalam data. from pandas import * from IPython.display import HTML , display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False )))","title":"Seleksi Fitur"},{"location":"seleksi_fitur/#contoh-data","text":"Data ini yang digunakan untuk melakukan seleksi fitur. Teknik yang digunakan adalah Information Gain. df = read_csv ( 'data.csv' , sep = ';' ) table ( df ) Outlook Temperature Humidity Windy Play Sunny Hot High False No Sunny Hot High True No Overcast Hot High False Yes Rainy Mild High False Yes Rainy Cool Normal False Yes Rainy Cool Normal True No Overcast Cool Normal True Yes Sunny Mild High False No Sunny Cool Normal False Yes Rainy Mild Normal False Yes Sunny Mild Normal True Yes Overcast Mild High True Yes Overcast Hot Normal False Yes Rainy Mild High True No","title":"Contoh Data"},{"location":"seleksi_fitur/#entropy","text":"Rumus mencari entropy adalah: $$ E(T)= \\sum_{i=1}^c - P_i log_2 P_i $$ def findEntropy ( column ): rawGroups = df . groupby ( column ) targetGroups = [[ key , len ( data ), len ( data ) / df [ column ] . size ] for key , data in rawGroups ] targetGroups = DataFrame ( targetGroups , columns = [ 'value' , 'count' , 'probability' ]) return sum ([ - x * log ( x , 2 ) for x in targetGroups [ 'probability' ]]), targetGroups , rawGroups entropyTarget , groupTargets , _ = findEntropy ( 'play' ) table ( groupTargets ) print ( 'entropy target =' , entropyTarget ) Value Count Probability No 5 0.357 Yes 9 0.643 Entropy target = 0.940","title":"Entropy"},{"location":"seleksi_fitur/#gain","text":"Information gain adalah salah satu metode untuk mendapatkan atribut-atribut yang paling berpengaruh terhadap dataset. Rumus dari Information Gain adalah: $$ Gain(T,X) = Entropy(T)-\\sum_{v \\in T}\\frac{T_{x,v}}{T} E(T_{x,v}) $$ def findGain ( column ): entropyOutlook , groupOutlooks , rawOutlooks = findEntropy ( column ) table ( groupOutlooks ) gain = entropyTarget - sum ( len ( data ) / len ( df ) * sum ( - x / len ( data ) * log ( x / len ( data ), 2 ) for x in data . groupby ( 'play' ) . size ()) for key , data in rawOutlooks ) print ( \"gain of\" , column , \"is\" , gain ) return gain gains = [[ x , findGain ( x )] for x in [ 'outlook' , 'temperature' , 'humidity' , 'windy' ]] Value Count Probability Overcast 4 0.286 Rainy 5 0.357 Sunny 5 0.357 Gain of Outlook is 0.247 Value Count Probability Cool 4 0.286 Hot 4 0.286 Mild 6 0.429 Gain of Temperature is 0.029 Value Count Probability High 7 0.5 Normal 7 0.5 Gain of Humidity is 0.152 Value Count Probability False 8 0.571 True 6 0.429 Gain of Windy is 0.048","title":"Gain"},{"location":"seleksi_fitur/#overall-gain-rank","text":"table ( DataFrame ( gains , columns = [ \"Feature\" , \"Gain Score\" ]) . sort_values ( \"Gain Score\" )[:: - 1 ]) Feature Gain Score Outlook 0.247 Humidity 0.152 Windy 0.048 Temperature 0.029","title":"Overall Gain Rank"},{"location":"seleksi_fitur/#hasil-akhir","text":"Dari seleksi fitur ini, setelah diurutkan berdasarkan nilai gain maka fitur yang tidak diperlukan lagi adalah Windy dan Temperature dan fitur yang digunakan hanya Outlook dan Humidity. Outlook Humidity Play Sunny High No Sunny High No Overcast High Yes Rainy High Yes Rainy Normal Yes Rainy Normal No Overcast Normal Yes Sunny High No Sunny Normal Yes Rainy Normal Yes Sunny Normal Yes Overcast High Yes Overcast Normal Yes Rainy High No","title":"Hasil Akhir"},{"location":"statistika/","text":"Statistika Data \u00b6 1. Rata-Rata \u00b6 Rata-rata atau disebut sebagai mean adalah suatu nilai hasil dari membagi jumlah nilai data dengan banyaknya data. Rata-rata menunjukkan pusat dari nilai data dan dapat mewakili dari keterpusatan data. Rata-rata merupakan suatu ukuran untuk memberikan gambaran yang lebih jelas dan singkat tentang sekumpulan data mengenai suatu persoalan. Rumusnya adalah: $$ \\begin{align} \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i \\end{align} $$ Bentuk programnya adalah: tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] jumlah = len ( tinggi ) total = 0 for angka in tinggi : total = total + angka mean = total / jumlah print ( \"Total nilai data adalah:\" , total ) print ( \"Rata-rata dari data adalah:\" , \" {:.2f} \" . format ( mean )) 2. Modus \u00b6 Modus adalah nilai yang mempunyai frekuensi terbesar dalam suatu kumpulan data. Modus berguna untuk mengetahui tingkat keseringan terjadinya suatu peristiwa. 3. Nilai Maksimum dan Minimum \u00b6 Nilai maksimum dan minimum digunakan untuk mencari nilai terbesar dan terkecil dari sebuah data. Program untuk mencari nilai maksimum dari data adalah: def maximum ( tinggi ): max_ = tinggi [ 0 ] for item in tinggi : if item > max_ : max_ = item return max_ tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] print ( \"Nilai maksimum dari data tersebut adalah:\" , maximum ( tinggi )) Program untuk mencari nilai minimum dari data adalah: def minimum ( tinggi ): min_ = tinggi [ 0 ] for item in tinggi : if item < min_ : min_ = item return min_ tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] print ( \"Nilai minimum dari data tersebut adalah:\" , minimum ( tinggi )) 4. Median \u00b6 Median adalah nilai tengah dari data yang telah disusun berurutan mulai dari yang terkecil sampai dengan yang terbesar. Rumusnya bila data tersebut ganjil adalah: $$ \\begin{align} Me = X\\left(\\frac{n+1}{2}\\right) \\end{align} $$ Rumusnya bila data tersebut genap adalah: $$ \\begin{align} Me = \\frac{1}{2}\\left(X(\\frac{n}{2})+X(\\frac{n}{2}+1)\\right) \\end{align} $$ def median ( lst ): sortedLst = sorted ( lst ) panjang_list = len ( lst ) index = ( panjang_list - 1 ) // 2 if ( panjang_list % 2 ): return sortedLst [ index ] else : return ( sortedLst [ index ] + sortedLst [ index + 1 ]) / 2.0 tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] print ( \"Median data ini adalah:\" , median ( tinggi )) 5. Kuartil \u00b6 Kuartil adalah Q1, Q2, dan Q3 yang membagi nilai-nilai pada data menjadi empat bagian sama rata. - Q1 atau kuartil pertama adalah membatasi 25 persen data. - Q2 atau kuartil kedua adalah membatasi 50 persen data. Kuartil kedua dapat juga disebut sebagai median atau nilai tengah data. - Q3 atau kuartil ketiga adalah membatasi 75 persen data. Rumus mencari kuartil data adalah: $$ \\begin{align} Q_1 = \\frac{1}{4}(n + 1) \\end{align} $$ $$ \\begin{align} Q_2 = \\frac{2}{4}(n + 1) \\end{align} $$ $$ \\begin{align} Q_3 = \\frac{3}{4}(n + 1) \\end{align} $$ tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] tinggi . sort () q1 = tinggi [ int ( len ( tinggi ) * 1 / 4 )] q2 = tinggi [ int ( len ( tinggi ) * 2 / 4 )] q3 = tinggi [ int ( len ( tinggi ) * 3 / 4 )] q4 = tinggi [ len ( tinggi ) - 1 ] print ( \"Datanya adalah:\" , tinggi ) print ( \"Data di kuartil 1 (Q1) adalah:\" , q1 ) print ( \"Data di kuartil 2 (Q2) adalah:\" , q2 ) print ( \"Data di kuartil 3 (Q3) adalah:\" , q3 ) 6. Standar Deviasi \u00b6 Standar deviasi adalah nilai statistik yang dimanfaatkan untuk menentukan bagaimana sebaran data dalam sampel, serta seberapa dekat titik data individu ke mean atau rata-rata nilai sampel. Sebuah standar deviasi dari kumpulan data sama dengan nol menandakan bahwa semua nilai dalam himpunan tersebut adalah sama. Sedangkan nilai deviasi yang lebih besar menunjukkan bahwa titik data individu jauh dari nilai rata-rata. Rumus mencari standar deviasi adalah: $$ \\begin{align} \\sigma = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2} \\end{align} $$ Contoh programnya adalah: tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] #Inisialisasi variabel jumlah_nilai = 0 jumlah_data = len ( tinggi ) #Menghitung jumlah nilai data for angka in tinggi : jumlah_nilai += angka #Menghitung rata-rata data mean = jumlah_nilai / jumlah_data #Inisialisasi variabel standard_dev dengan nilai 0 standard_dev = 0 #Menghitung nilai untuk mencari standar deviasi for elemen in tinggi : standard_dev += ( float ( elemen ) - mean ) ** 2 #Menghitung nilai akhir standar deviasi standard_dev = ( standard_dev * 1 / jumlah_data ) ** ( 1 / 2 ) print ( \"Standar deviasinya adalah:\" , \" {:.2f} \" . format ( standard_dev )) 7. Kemencengan ( Skewness ) \u00b6 Ukuran kemiringan adalah ukuran yang menyatakan derajat ketidaksimetrisan suatu lengkungan halus (kurva) dari suatu distribusi frekuensi. Kemiringan distribusi data ada tiga jenis, yaitu simetris, menceng ke kanan yang berarti kemiringannya positif, dan menceng ke kiri yang berarti kemiringannya negatif. Rumus mencari kemencengan adalah: $$ sk = \\frac {\\bar{X} - Mo}{s} $$ Contoh Program untuk Sebuah Data \u00b6 Program ini menggunakan modul Pandas dan Stats, untuk membuat tampilan tabel menggunakan HTML, display, dan tabulate. Data yang digunakan adalah: import pandas as pd from scipy import stats df = pd . read_csv ( \"data.csv\" ) from IPython.display import HTML , display import tabulate table = [ [ \"method\" ] + [ x for x in df . columns ], [ \"describe()\" ] + [ \"<pre>\" + str ( df [ col ] . describe ()) + \"</pre>\" for col in df . columns ], [ \"count()\" ] + [ df [ col ] . count () for col in df . columns ], [ \"mean()\" ] + [ df [ col ] . mean () for col in df . columns ], [ \"std()\" ] + [ \" {:.2f} \" . format ( df [ col ] . std ()) for col in df . columns ], [ \"max()\" ] + [ df [ col ] . max () for col in df . columns ], [ \"min()\" ] + [ df [ col ] . min () for col in df . columns ], [ \"q1()\" ] + [ df [ col ] . quantile ( 0.25 ) for col in df . columns ], [ \"q2()\" ] + [ df [ col ] . quantile ( 0.50 ) for col in df . columns ], [ \"q3()\" ] + [ df [ col ] . quantile ( 0.75 ) for col in df . columns ], [ \"skew()\" ] + [ \" {:.2f} \" . format ( df [ col ] . skew ()) for col in df . columns ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = \"html\" )))","title":"Statistika"},{"location":"statistika/#statistika-data","text":"","title":"Statistika Data"},{"location":"statistika/#1-rata-rata","text":"Rata-rata atau disebut sebagai mean adalah suatu nilai hasil dari membagi jumlah nilai data dengan banyaknya data. Rata-rata menunjukkan pusat dari nilai data dan dapat mewakili dari keterpusatan data. Rata-rata merupakan suatu ukuran untuk memberikan gambaran yang lebih jelas dan singkat tentang sekumpulan data mengenai suatu persoalan. Rumusnya adalah: $$ \\begin{align} \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i \\end{align} $$ Bentuk programnya adalah: tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] jumlah = len ( tinggi ) total = 0 for angka in tinggi : total = total + angka mean = total / jumlah print ( \"Total nilai data adalah:\" , total ) print ( \"Rata-rata dari data adalah:\" , \" {:.2f} \" . format ( mean ))","title":"1. Rata-Rata"},{"location":"statistika/#2-modus","text":"Modus adalah nilai yang mempunyai frekuensi terbesar dalam suatu kumpulan data. Modus berguna untuk mengetahui tingkat keseringan terjadinya suatu peristiwa.","title":"2. Modus"},{"location":"statistika/#3-nilai-maksimum-dan-minimum","text":"Nilai maksimum dan minimum digunakan untuk mencari nilai terbesar dan terkecil dari sebuah data. Program untuk mencari nilai maksimum dari data adalah: def maximum ( tinggi ): max_ = tinggi [ 0 ] for item in tinggi : if item > max_ : max_ = item return max_ tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] print ( \"Nilai maksimum dari data tersebut adalah:\" , maximum ( tinggi )) Program untuk mencari nilai minimum dari data adalah: def minimum ( tinggi ): min_ = tinggi [ 0 ] for item in tinggi : if item < min_ : min_ = item return min_ tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] print ( \"Nilai minimum dari data tersebut adalah:\" , minimum ( tinggi ))","title":"3. Nilai Maksimum dan Minimum"},{"location":"statistika/#4-median","text":"Median adalah nilai tengah dari data yang telah disusun berurutan mulai dari yang terkecil sampai dengan yang terbesar. Rumusnya bila data tersebut ganjil adalah: $$ \\begin{align} Me = X\\left(\\frac{n+1}{2}\\right) \\end{align} $$ Rumusnya bila data tersebut genap adalah: $$ \\begin{align} Me = \\frac{1}{2}\\left(X(\\frac{n}{2})+X(\\frac{n}{2}+1)\\right) \\end{align} $$ def median ( lst ): sortedLst = sorted ( lst ) panjang_list = len ( lst ) index = ( panjang_list - 1 ) // 2 if ( panjang_list % 2 ): return sortedLst [ index ] else : return ( sortedLst [ index ] + sortedLst [ index + 1 ]) / 2.0 tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] print ( \"Median data ini adalah:\" , median ( tinggi ))","title":"4. Median"},{"location":"statistika/#5-kuartil","text":"Kuartil adalah Q1, Q2, dan Q3 yang membagi nilai-nilai pada data menjadi empat bagian sama rata. - Q1 atau kuartil pertama adalah membatasi 25 persen data. - Q2 atau kuartil kedua adalah membatasi 50 persen data. Kuartil kedua dapat juga disebut sebagai median atau nilai tengah data. - Q3 atau kuartil ketiga adalah membatasi 75 persen data. Rumus mencari kuartil data adalah: $$ \\begin{align} Q_1 = \\frac{1}{4}(n + 1) \\end{align} $$ $$ \\begin{align} Q_2 = \\frac{2}{4}(n + 1) \\end{align} $$ $$ \\begin{align} Q_3 = \\frac{3}{4}(n + 1) \\end{align} $$ tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] tinggi . sort () q1 = tinggi [ int ( len ( tinggi ) * 1 / 4 )] q2 = tinggi [ int ( len ( tinggi ) * 2 / 4 )] q3 = tinggi [ int ( len ( tinggi ) * 3 / 4 )] q4 = tinggi [ len ( tinggi ) - 1 ] print ( \"Datanya adalah:\" , tinggi ) print ( \"Data di kuartil 1 (Q1) adalah:\" , q1 ) print ( \"Data di kuartil 2 (Q2) adalah:\" , q2 ) print ( \"Data di kuartil 3 (Q3) adalah:\" , q3 )","title":"5. Kuartil"},{"location":"statistika/#6-standar-deviasi","text":"Standar deviasi adalah nilai statistik yang dimanfaatkan untuk menentukan bagaimana sebaran data dalam sampel, serta seberapa dekat titik data individu ke mean atau rata-rata nilai sampel. Sebuah standar deviasi dari kumpulan data sama dengan nol menandakan bahwa semua nilai dalam himpunan tersebut adalah sama. Sedangkan nilai deviasi yang lebih besar menunjukkan bahwa titik data individu jauh dari nilai rata-rata. Rumus mencari standar deviasi adalah: $$ \\begin{align} \\sigma = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2} \\end{align} $$ Contoh programnya adalah: tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] #Inisialisasi variabel jumlah_nilai = 0 jumlah_data = len ( tinggi ) #Menghitung jumlah nilai data for angka in tinggi : jumlah_nilai += angka #Menghitung rata-rata data mean = jumlah_nilai / jumlah_data #Inisialisasi variabel standard_dev dengan nilai 0 standard_dev = 0 #Menghitung nilai untuk mencari standar deviasi for elemen in tinggi : standard_dev += ( float ( elemen ) - mean ) ** 2 #Menghitung nilai akhir standar deviasi standard_dev = ( standard_dev * 1 / jumlah_data ) ** ( 1 / 2 ) print ( \"Standar deviasinya adalah:\" , \" {:.2f} \" . format ( standard_dev ))","title":"6. Standar Deviasi"},{"location":"statistika/#7-kemencengan-skewness","text":"Ukuran kemiringan adalah ukuran yang menyatakan derajat ketidaksimetrisan suatu lengkungan halus (kurva) dari suatu distribusi frekuensi. Kemiringan distribusi data ada tiga jenis, yaitu simetris, menceng ke kanan yang berarti kemiringannya positif, dan menceng ke kiri yang berarti kemiringannya negatif. Rumus mencari kemencengan adalah: $$ sk = \\frac {\\bar{X} - Mo}{s} $$","title":"7. Kemencengan (Skewness)"},{"location":"statistika/#contoh-program-untuk-sebuah-data","text":"Program ini menggunakan modul Pandas dan Stats, untuk membuat tampilan tabel menggunakan HTML, display, dan tabulate. Data yang digunakan adalah: import pandas as pd from scipy import stats df = pd . read_csv ( \"data.csv\" ) from IPython.display import HTML , display import tabulate table = [ [ \"method\" ] + [ x for x in df . columns ], [ \"describe()\" ] + [ \"<pre>\" + str ( df [ col ] . describe ()) + \"</pre>\" for col in df . columns ], [ \"count()\" ] + [ df [ col ] . count () for col in df . columns ], [ \"mean()\" ] + [ df [ col ] . mean () for col in df . columns ], [ \"std()\" ] + [ \" {:.2f} \" . format ( df [ col ] . std ()) for col in df . columns ], [ \"max()\" ] + [ df [ col ] . max () for col in df . columns ], [ \"min()\" ] + [ df [ col ] . min () for col in df . columns ], [ \"q1()\" ] + [ df [ col ] . quantile ( 0.25 ) for col in df . columns ], [ \"q2()\" ] + [ df [ col ] . quantile ( 0.50 ) for col in df . columns ], [ \"q3()\" ] + [ df [ col ] . quantile ( 0.75 ) for col in df . columns ], [ \"skew()\" ] + [ \" {:.2f} \" . format ( df [ col ] . skew ()) for col in df . columns ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = \"html\" )))","title":"Contoh Program untuk Sebuah Data"},{"location":"weighted_knn/","text":"Weighted K-Nearest Neighbor \u00b6 Data Uji dan Data Sample \u00b6 Data yang Diujikan \u00b6 Data yang digunakan adalah data bunga Iris. sepal_length sepal_width petal_length petal_width class 5.1 3.5 1.4 0.2 Iris-setosa 4.9 3.0 1.4 0.2 Iris-setosa 4.7 3.2 1.3 0.2 Iris-setosa 4.6 3.1 1.5 0.2 Iris-setosa 5.0 3.6 1.4 0.2 Iris-setosa 5.4 3.9 1.7 0.4 Iris-setosa 4.6 3.4 1.4 0.3 Iris-setosa 5.0 3.4 1.5 0.2 Iris-setosa 4.4 2.9 1.4 0.2 Iris-setosa 4.9 3.1 1.5 0.1 Iris-setosa 5.4 3.7 1.5 0.2 Iris-setosa 4.8 3.4 1.6 0.2 Iris-setosa 4.8 3.0 1.4 0.1 Iris-setosa 4.3 3.0 1.1 0.1 Iris-setosa 5.8 4.0 1.2 0.2 Iris-setosa 5.7 4.4 1.5 0.4 Iris-setosa 5.4 3.9 1.3 0.4 Iris-setosa 5.1 3.5 1.4 0.3 Iris-setosa 5.7 3.8 1.7 0.3 Iris-setosa 5.1 3.8 1.5 0.3 Iris-setosa 5.4 3.4 1.7 0.2 Iris-setosa 5.1 3.7 1.5 0.4 Iris-setosa 4.6 3.6 1.0 0.2 Iris-setosa 5.1 3.3 1.7 0.5 Iris-setosa 4.8 3.4 1.9 0.2 Iris-setosa 5.0 3.0 1.6 0.2 Iris-setosa 5.0 3.4 1.6 0.4 Iris-setosa 5.2 3.5 1.5 0.2 Iris-setosa 5.2 3.4 1.4 0.2 Iris-setosa 4.7 3.2 1.6 0.2 Iris-setosa 4.8 3.1 1.6 0.2 Iris-setosa 5.4 3.4 1.5 0.4 Iris-setosa 5.2 4.1 1.5 0.1 Iris-setosa 5.5 4.2 1.4 0.2 Iris-setosa 4.9 3.1 1.5 0.1 Iris-setosa 5.0 3.2 1.2 0.2 Iris-setosa 5.5 3.5 1.3 0.2 Iris-setosa 4.9 3.1 1.5 0.1 Iris-setosa 4.4 3.0 1.3 0.2 Iris-setosa 5.1 3.4 1.5 0.2 Iris-setosa 7.0 3.2 4.7 1.4 Iris-versicolor 6.4 3.2 4.5 1.5 Iris-versicolor 6.9 3.1 4.9 1.5 Iris-versicolor 5.5 2.3 4.0 1.3 Iris-versicolor 6.5 2.8 4.6 1.5 Iris-versicolor 5.7 2.8 4.5 1.3 Iris-versicolor 6.3 3.3 4.7 1.6 Iris-versicolor 4.9 2.4 3.3 1.0 Iris-versicolor 6.6 2.9 4.6 1.3 Iris-versicolor 5.2 2.7 3.9 1.4 Iris-versicolor 5.0 2.0 3.5 1.0 Iris-versicolor 5.9 3.0 4.2 1.5 Iris-versicolor 6.0 2.2 4.0 1.0 Iris-versicolor 6.1 2.9 4.7 1.4 Iris-versicolor 5.6 2.9 3.6 1.3 Iris-versicolor 6.7 3.1 4.4 1.4 Iris-versicolor 5.6 3.0 4.5 1.5 Iris-versicolor 5.8 2.7 4.1 1.0 Iris-versicolor 6.2 2.2 4.5 1.5 Iris-versicolor 5.6 2.5 3.9 1.1 Iris-versicolor 5.9 3.2 4.8 1.8 Iris-versicolor 6.1 2.8 4.0 1.3 Iris-versicolor 6.3 2.5 4.9 1.5 Iris-versicolor 6.1 2.8 4.7 1.2 Iris-versicolor 6.4 2.9 4.3 1.3 Iris-versicolor 6.6 3.0 4.4 1.4 Iris-versicolor 6.8 2.8 4.8 1.4 Iris-versicolor 6.7 3.0 5.0 1.7 Iris-versicolor 6.0 2.9 4.5 1.5 Iris-versicolor 5.7 2.6 3.5 1.0 Iris-versicolor 6.3 3.3 6.0 2.5 Iris-virginica 5.8 2.7 5.1 1.9 Iris-virginica 7.1 3.0 5.9 2.1 Iris-virginica 6.3 2.9 5.6 1.8 Iris-virginica 6.5 3.0 5.8 2.2 Iris-virginica 7.6 3.0 6.6 2.1 Iris-virginica 4.9 2.5 4.5 1.7 Iris-virginica 7.3 2.9 6.3 1.8 Iris-virginica 6.7 2.5 5.8 1.8 Iris-virginica 7.2 3.6 6.1 2.5 Iris-virginica 6.5 3.2 5.1 2.0 Iris-virginica 6.4 2.7 5.3 1.9 Iris-virginica 6.8 3.0 5.5 2.1 Iris-virginica 5.7 2.5 5.0 2.0 Iris-virginica 5.8 2.8 5.1 2.4 Iris-virginica 6.4 3.2 5.3 2.3 Iris-virginica 6.5 3.0 5.5 1.8 Iris-virginica 7.7 3.8 6.7 2.2 Iris-virginica 7.7 2.6 6.9 2.3 Iris-virginica 6.0 2.2 5.0 1.5 Iris-virginica 6.9 3.2 5.7 2.3 Iris-virginica 5.6 2.8 4.9 2.0 Iris-virginica 7.7 2.8 6.7 2.0 Iris-virginica 6.3 2.7 4.9 1.8 Iris-virginica 6.7 3.3 5.7 2.1 Iris-virginica 7.2 3.2 6.0 1.8 Iris-virginica 6.2 2.8 4.8 1.8 Iris-virginica 6.1 3.0 4.9 1.8 Iris-virginica 6.4 2.8 5.6 2.1 Iris-virginica 7.2 3.0 5.8 1.6 Iris-virginica Data Sample \u00b6 sepal_length sepal_width petal_length petal_width class 7.9 3.8 6.4 2.0 Iris-virginica Menghitung Jarak Data Uji Terhadap Data Sampel \u00b6 Karena data yang diuji adalah data numerik, maka menggunakan Euclidean Distance: $$ d (p,q) = \\sqrt{(q_1-p_1)^2 + (q_2-p_2)^2} $$ sepal_length sepal_width petal_length petal_width class jarak 7,7 3,8 6,7 2,2 Iris-virginica 0,4 7,6 3,0 6,6 2,1 Iris-virginica 0,9 7,2 3,6 6,1 2,5 Iris-virginica 0,9 7,2 3,2 6,0 1,8 Iris-virginica 1,0 7,7 2,8 6,7 2,0 Iris-virginica 1,1 7,3 2,9 6,3 1,8 Iris-virginica 1,1 7,1 3,0 5,9 2,1 Iris-virginica 1,2 7,2 3,0 5,8 1,6 Iris-virginica 1,3 7,7 2,6 6,9 2,3 Iris-virginica 1,3 6,9 3,2 5,7 2,3 Iris-virginica 1,4 6,7 3,3 5,7 2,1 Iris-virginica 1,5 6,8 3,0 5,5 2,1 Iris-virginica 1,6 6,5 3,0 5,8 2,2 Iris-virginica 1,7 6,3 3,3 6,0 2,5 Iris-virginica 1,8 6,5 3,0 5,5 1,8 Iris-virginica 1,9 6,7 2,5 5,8 1,8 Iris-virginica 1,9 6,4 2,8 5,6 2,1 Iris-virginica 2,0 6,4 3,2 5,3 2,3 Iris-virginica 2,0 6,9 3,1 4,9 1,5 Iris-versicolor 2,0 6,5 3,2 5,1 2,0 Iris-virginica 2,0 6,3 2,9 5,6 1,8 Iris-virginica 2,0 6,7 3,0 5,0 1,7 Iris-versicolor 2,0 7,0 3,2 4,7 1,4 Iris-versicolor 2,1 6,4 2,7 5,3 1,9 Iris-virginica 2,2 6,8 2,8 4,8 1,4 Iris-versicolor 2,3 6,3 3,3 4,7 1,6 Iris-versicolor 2,4 6,3 2,7 4,9 1,8 Iris-virginica 2,5 6,1 3,0 4,9 1,8 Iris-virginica 2,5 6,6 2,9 4,6 1,3 Iris-versicolor 2,5 6,7 3,1 4,4 1,4 Iris-versicolor 2,5 6,5 2,8 4,6 1,5 Iris-versicolor 2,5 6,4 3,2 4,5 1,5 Iris-versicolor 2,5 6,2 2,8 4,8 1,8 Iris-virginica 2,5 6,6 3,0 4,4 1,4 Iris-versicolor 2,6 6,3 2,5 4,9 1,5 Iris-versicolor 2,6 5,9 3,2 4,8 1,8 Iris-versicolor 2,6 5,8 2,8 5,1 2,4 Iris-virginica 2,7 6,1 2,9 4,7 1,4 Iris-versicolor 2,7 5,8 2,7 5,1 1,9 Iris-virginica 2,7 6,1 2,8 4,7 1,2 Iris-versicolor 2,8 6,4 2,9 4,3 1,3 Iris-versicolor 2,8 6,0 2,9 4,5 1,5 Iris-versicolor 2,9 6,0 2,2 5,0 1,5 Iris-virginica 2,9 5,7 2,5 5,0 2,0 Iris-virginica 2,9 5,6 2,8 4,9 2,0 Iris-virginica 2,9 6,2 2,2 4,5 1,5 Iris-versicolor 3,1 5,9 3,0 4,2 1,5 Iris-versicolor 3,1 5,6 3,0 4,5 1,5 Iris-versicolor 3,1 5,7 2,8 4,5 1,3 Iris-versicolor 3,2 6,1 2,8 4,0 1,3 Iris-versicolor 3,2 5,8 2,7 4,1 1,0 Iris-versicolor 3,5 6,0 2,2 4,0 1,0 Iris-versicolor 3,6 5,6 2,5 3,9 1,1 Iris-versicolor 3,7 5,5 2,3 4,0 1,3 Iris-versicolor 3,8 4,9 2,5 4,5 1,7 Iris-virginica 3,8 5,6 2,9 3,6 1,3 Iris-versicolor 3,8 5,2 2,7 3,9 1,4 Iris-versicolor 3,9 5,7 2,6 3,5 1,0 Iris-versicolor 4,0 5,0 2,0 3,5 1,0 Iris-versicolor 4,6 4,9 2,4 3,3 1,0 Iris-versicolor 4,6 5,7 3,8 1,7 0,3 Iris-setosa 5,5 5,4 3,9 1,7 0,4 Iris-setosa 5,6 5,4 3,4 1,7 0,2 Iris-setosa 5,6 5,7 4,4 1,5 0,4 Iris-setosa 5,6 5,1 3,3 1,7 0,5 Iris-setosa 5,7 5,4 3,4 1,5 0,4 Iris-setosa 5,7 4,8 3,4 1,9 0,2 Iris-setosa 5,8 5,4 3,7 1,5 0,2 Iris-setosa 5,8 5,5 4,2 1,4 0,2 Iris-setosa 5,8 5,0 3,4 1,6 0,4 Iris-setosa 5,8 5,1 3,7 1,5 0,4 Iris-setosa 5,9 5,2 3,5 1,5 0,2 Iris-setosa 5,9 5,8 4,0 1,2 0,2 Iris-setosa 5,9 5,1 3,8 1,5 0,3 Iris-setosa 5,9 5,4 3,9 1,3 0,4 Iris-setosa 5,9 5,2 4,1 1,5 0,1 Iris-setosa 5,9 5,5 3,5 1,3 0,2 Iris-setosa 5,9 5,1 3,4 1,5 0,2 Iris-setosa 5,9 5,0 3,0 1,6 0,2 Iris-setosa 5,9 5,2 3,4 1,4 0,2 Iris-setosa 6,0 5,0 3,4 1,5 0,2 Iris-setosa 6,0 5,1 3,5 1,4 0,3 Iris-setosa 6,0 4,8 3,4 1,6 0,2 Iris-setosa 6,0 5,1 3,5 1,4 0,2 Iris-setosa 6,0 4,8 3,1 1,6 0,2 Iris-setosa 6,0 5,0 3,6 1,4 0,2 Iris-setosa 6,1 4,7 3,2 1,6 0,2 Iris-setosa 6,1 4,9 3,1 1,5 0,1 Iris-setosa 6,1 4,9 3,1 1,5 0,1 Iris-setosa 6,1 4,9 3,1 1,5 0,1 Iris-setosa 6,1 4,9 3,0 1,4 0,2 Iris-setosa 6,2 4,6 3,1 1,5 0,2 Iris-setosa 6,2 4,8 3,0 1,4 0,1 Iris-setosa 6,2 4,6 3,4 1,4 0,3 Iris-setosa 6,2 5,0 3,2 1,2 0,2 Iris-setosa 6,2 4,7 3,2 1,3 0,2 Iris-setosa 6,3 4,4 2,9 1,4 0,2 Iris-setosa 6,4 4,4 3,0 1,3 0,2 Iris-setosa 6,5 4,6 3,6 1,0 0,2 Iris-setosa 6,6 4,3 3,0 1,1 0,1 Iris-setosa 6,7 Dirangking untuk k-5 teratas \u00b6 sepal_length sepal_width petal_length petal_width class jarak 7,7 3,8 6,7 2,2 Iris-virginica 0,4 7,6 3,0 6,6 2,1 Iris-virginica 0,9 7,2 3,6 6,1 2,5 Iris-virginica 0,9 7,2 3,2 6,0 1,8 Iris-virginica 1,0 7,7 2,8 6,7 2,0 Iris-virginica 1,1 Menghitung Berat Antar Variasi \u00b6 Menghitung berat antar variasi dilakukan dengan rumus 1/jarak. Jarak 1/Jarak Setosa Virginica Versicolor 0.4 2.4 0 2.4 0 0.9 1.1 0 1.1 0 0.9 1.1 0 1.1 0 1.0 1.0 0 1.0 0 1.1 0.9 0 0.9 0 Jumlah 0 5.6 0 Kesimpulan \u00b6 Nilai terbesar adalah = Iris-Virginica dengan Nilai 5.6","title":"Weighted KNN"},{"location":"weighted_knn/#weighted-k-nearest-neighbor","text":"","title":"Weighted K-Nearest Neighbor"},{"location":"weighted_knn/#data-uji-dan-data-sample","text":"","title":"Data Uji dan Data Sample"},{"location":"weighted_knn/#data-yang-diujikan","text":"Data yang digunakan adalah data bunga Iris. sepal_length sepal_width petal_length petal_width class 5.1 3.5 1.4 0.2 Iris-setosa 4.9 3.0 1.4 0.2 Iris-setosa 4.7 3.2 1.3 0.2 Iris-setosa 4.6 3.1 1.5 0.2 Iris-setosa 5.0 3.6 1.4 0.2 Iris-setosa 5.4 3.9 1.7 0.4 Iris-setosa 4.6 3.4 1.4 0.3 Iris-setosa 5.0 3.4 1.5 0.2 Iris-setosa 4.4 2.9 1.4 0.2 Iris-setosa 4.9 3.1 1.5 0.1 Iris-setosa 5.4 3.7 1.5 0.2 Iris-setosa 4.8 3.4 1.6 0.2 Iris-setosa 4.8 3.0 1.4 0.1 Iris-setosa 4.3 3.0 1.1 0.1 Iris-setosa 5.8 4.0 1.2 0.2 Iris-setosa 5.7 4.4 1.5 0.4 Iris-setosa 5.4 3.9 1.3 0.4 Iris-setosa 5.1 3.5 1.4 0.3 Iris-setosa 5.7 3.8 1.7 0.3 Iris-setosa 5.1 3.8 1.5 0.3 Iris-setosa 5.4 3.4 1.7 0.2 Iris-setosa 5.1 3.7 1.5 0.4 Iris-setosa 4.6 3.6 1.0 0.2 Iris-setosa 5.1 3.3 1.7 0.5 Iris-setosa 4.8 3.4 1.9 0.2 Iris-setosa 5.0 3.0 1.6 0.2 Iris-setosa 5.0 3.4 1.6 0.4 Iris-setosa 5.2 3.5 1.5 0.2 Iris-setosa 5.2 3.4 1.4 0.2 Iris-setosa 4.7 3.2 1.6 0.2 Iris-setosa 4.8 3.1 1.6 0.2 Iris-setosa 5.4 3.4 1.5 0.4 Iris-setosa 5.2 4.1 1.5 0.1 Iris-setosa 5.5 4.2 1.4 0.2 Iris-setosa 4.9 3.1 1.5 0.1 Iris-setosa 5.0 3.2 1.2 0.2 Iris-setosa 5.5 3.5 1.3 0.2 Iris-setosa 4.9 3.1 1.5 0.1 Iris-setosa 4.4 3.0 1.3 0.2 Iris-setosa 5.1 3.4 1.5 0.2 Iris-setosa 7.0 3.2 4.7 1.4 Iris-versicolor 6.4 3.2 4.5 1.5 Iris-versicolor 6.9 3.1 4.9 1.5 Iris-versicolor 5.5 2.3 4.0 1.3 Iris-versicolor 6.5 2.8 4.6 1.5 Iris-versicolor 5.7 2.8 4.5 1.3 Iris-versicolor 6.3 3.3 4.7 1.6 Iris-versicolor 4.9 2.4 3.3 1.0 Iris-versicolor 6.6 2.9 4.6 1.3 Iris-versicolor 5.2 2.7 3.9 1.4 Iris-versicolor 5.0 2.0 3.5 1.0 Iris-versicolor 5.9 3.0 4.2 1.5 Iris-versicolor 6.0 2.2 4.0 1.0 Iris-versicolor 6.1 2.9 4.7 1.4 Iris-versicolor 5.6 2.9 3.6 1.3 Iris-versicolor 6.7 3.1 4.4 1.4 Iris-versicolor 5.6 3.0 4.5 1.5 Iris-versicolor 5.8 2.7 4.1 1.0 Iris-versicolor 6.2 2.2 4.5 1.5 Iris-versicolor 5.6 2.5 3.9 1.1 Iris-versicolor 5.9 3.2 4.8 1.8 Iris-versicolor 6.1 2.8 4.0 1.3 Iris-versicolor 6.3 2.5 4.9 1.5 Iris-versicolor 6.1 2.8 4.7 1.2 Iris-versicolor 6.4 2.9 4.3 1.3 Iris-versicolor 6.6 3.0 4.4 1.4 Iris-versicolor 6.8 2.8 4.8 1.4 Iris-versicolor 6.7 3.0 5.0 1.7 Iris-versicolor 6.0 2.9 4.5 1.5 Iris-versicolor 5.7 2.6 3.5 1.0 Iris-versicolor 6.3 3.3 6.0 2.5 Iris-virginica 5.8 2.7 5.1 1.9 Iris-virginica 7.1 3.0 5.9 2.1 Iris-virginica 6.3 2.9 5.6 1.8 Iris-virginica 6.5 3.0 5.8 2.2 Iris-virginica 7.6 3.0 6.6 2.1 Iris-virginica 4.9 2.5 4.5 1.7 Iris-virginica 7.3 2.9 6.3 1.8 Iris-virginica 6.7 2.5 5.8 1.8 Iris-virginica 7.2 3.6 6.1 2.5 Iris-virginica 6.5 3.2 5.1 2.0 Iris-virginica 6.4 2.7 5.3 1.9 Iris-virginica 6.8 3.0 5.5 2.1 Iris-virginica 5.7 2.5 5.0 2.0 Iris-virginica 5.8 2.8 5.1 2.4 Iris-virginica 6.4 3.2 5.3 2.3 Iris-virginica 6.5 3.0 5.5 1.8 Iris-virginica 7.7 3.8 6.7 2.2 Iris-virginica 7.7 2.6 6.9 2.3 Iris-virginica 6.0 2.2 5.0 1.5 Iris-virginica 6.9 3.2 5.7 2.3 Iris-virginica 5.6 2.8 4.9 2.0 Iris-virginica 7.7 2.8 6.7 2.0 Iris-virginica 6.3 2.7 4.9 1.8 Iris-virginica 6.7 3.3 5.7 2.1 Iris-virginica 7.2 3.2 6.0 1.8 Iris-virginica 6.2 2.8 4.8 1.8 Iris-virginica 6.1 3.0 4.9 1.8 Iris-virginica 6.4 2.8 5.6 2.1 Iris-virginica 7.2 3.0 5.8 1.6 Iris-virginica","title":"Data yang Diujikan"},{"location":"weighted_knn/#data-sample","text":"sepal_length sepal_width petal_length petal_width class 7.9 3.8 6.4 2.0 Iris-virginica","title":"Data Sample"},{"location":"weighted_knn/#menghitung-jarak-data-uji-terhadap-data-sampel","text":"Karena data yang diuji adalah data numerik, maka menggunakan Euclidean Distance: $$ d (p,q) = \\sqrt{(q_1-p_1)^2 + (q_2-p_2)^2} $$ sepal_length sepal_width petal_length petal_width class jarak 7,7 3,8 6,7 2,2 Iris-virginica 0,4 7,6 3,0 6,6 2,1 Iris-virginica 0,9 7,2 3,6 6,1 2,5 Iris-virginica 0,9 7,2 3,2 6,0 1,8 Iris-virginica 1,0 7,7 2,8 6,7 2,0 Iris-virginica 1,1 7,3 2,9 6,3 1,8 Iris-virginica 1,1 7,1 3,0 5,9 2,1 Iris-virginica 1,2 7,2 3,0 5,8 1,6 Iris-virginica 1,3 7,7 2,6 6,9 2,3 Iris-virginica 1,3 6,9 3,2 5,7 2,3 Iris-virginica 1,4 6,7 3,3 5,7 2,1 Iris-virginica 1,5 6,8 3,0 5,5 2,1 Iris-virginica 1,6 6,5 3,0 5,8 2,2 Iris-virginica 1,7 6,3 3,3 6,0 2,5 Iris-virginica 1,8 6,5 3,0 5,5 1,8 Iris-virginica 1,9 6,7 2,5 5,8 1,8 Iris-virginica 1,9 6,4 2,8 5,6 2,1 Iris-virginica 2,0 6,4 3,2 5,3 2,3 Iris-virginica 2,0 6,9 3,1 4,9 1,5 Iris-versicolor 2,0 6,5 3,2 5,1 2,0 Iris-virginica 2,0 6,3 2,9 5,6 1,8 Iris-virginica 2,0 6,7 3,0 5,0 1,7 Iris-versicolor 2,0 7,0 3,2 4,7 1,4 Iris-versicolor 2,1 6,4 2,7 5,3 1,9 Iris-virginica 2,2 6,8 2,8 4,8 1,4 Iris-versicolor 2,3 6,3 3,3 4,7 1,6 Iris-versicolor 2,4 6,3 2,7 4,9 1,8 Iris-virginica 2,5 6,1 3,0 4,9 1,8 Iris-virginica 2,5 6,6 2,9 4,6 1,3 Iris-versicolor 2,5 6,7 3,1 4,4 1,4 Iris-versicolor 2,5 6,5 2,8 4,6 1,5 Iris-versicolor 2,5 6,4 3,2 4,5 1,5 Iris-versicolor 2,5 6,2 2,8 4,8 1,8 Iris-virginica 2,5 6,6 3,0 4,4 1,4 Iris-versicolor 2,6 6,3 2,5 4,9 1,5 Iris-versicolor 2,6 5,9 3,2 4,8 1,8 Iris-versicolor 2,6 5,8 2,8 5,1 2,4 Iris-virginica 2,7 6,1 2,9 4,7 1,4 Iris-versicolor 2,7 5,8 2,7 5,1 1,9 Iris-virginica 2,7 6,1 2,8 4,7 1,2 Iris-versicolor 2,8 6,4 2,9 4,3 1,3 Iris-versicolor 2,8 6,0 2,9 4,5 1,5 Iris-versicolor 2,9 6,0 2,2 5,0 1,5 Iris-virginica 2,9 5,7 2,5 5,0 2,0 Iris-virginica 2,9 5,6 2,8 4,9 2,0 Iris-virginica 2,9 6,2 2,2 4,5 1,5 Iris-versicolor 3,1 5,9 3,0 4,2 1,5 Iris-versicolor 3,1 5,6 3,0 4,5 1,5 Iris-versicolor 3,1 5,7 2,8 4,5 1,3 Iris-versicolor 3,2 6,1 2,8 4,0 1,3 Iris-versicolor 3,2 5,8 2,7 4,1 1,0 Iris-versicolor 3,5 6,0 2,2 4,0 1,0 Iris-versicolor 3,6 5,6 2,5 3,9 1,1 Iris-versicolor 3,7 5,5 2,3 4,0 1,3 Iris-versicolor 3,8 4,9 2,5 4,5 1,7 Iris-virginica 3,8 5,6 2,9 3,6 1,3 Iris-versicolor 3,8 5,2 2,7 3,9 1,4 Iris-versicolor 3,9 5,7 2,6 3,5 1,0 Iris-versicolor 4,0 5,0 2,0 3,5 1,0 Iris-versicolor 4,6 4,9 2,4 3,3 1,0 Iris-versicolor 4,6 5,7 3,8 1,7 0,3 Iris-setosa 5,5 5,4 3,9 1,7 0,4 Iris-setosa 5,6 5,4 3,4 1,7 0,2 Iris-setosa 5,6 5,7 4,4 1,5 0,4 Iris-setosa 5,6 5,1 3,3 1,7 0,5 Iris-setosa 5,7 5,4 3,4 1,5 0,4 Iris-setosa 5,7 4,8 3,4 1,9 0,2 Iris-setosa 5,8 5,4 3,7 1,5 0,2 Iris-setosa 5,8 5,5 4,2 1,4 0,2 Iris-setosa 5,8 5,0 3,4 1,6 0,4 Iris-setosa 5,8 5,1 3,7 1,5 0,4 Iris-setosa 5,9 5,2 3,5 1,5 0,2 Iris-setosa 5,9 5,8 4,0 1,2 0,2 Iris-setosa 5,9 5,1 3,8 1,5 0,3 Iris-setosa 5,9 5,4 3,9 1,3 0,4 Iris-setosa 5,9 5,2 4,1 1,5 0,1 Iris-setosa 5,9 5,5 3,5 1,3 0,2 Iris-setosa 5,9 5,1 3,4 1,5 0,2 Iris-setosa 5,9 5,0 3,0 1,6 0,2 Iris-setosa 5,9 5,2 3,4 1,4 0,2 Iris-setosa 6,0 5,0 3,4 1,5 0,2 Iris-setosa 6,0 5,1 3,5 1,4 0,3 Iris-setosa 6,0 4,8 3,4 1,6 0,2 Iris-setosa 6,0 5,1 3,5 1,4 0,2 Iris-setosa 6,0 4,8 3,1 1,6 0,2 Iris-setosa 6,0 5,0 3,6 1,4 0,2 Iris-setosa 6,1 4,7 3,2 1,6 0,2 Iris-setosa 6,1 4,9 3,1 1,5 0,1 Iris-setosa 6,1 4,9 3,1 1,5 0,1 Iris-setosa 6,1 4,9 3,1 1,5 0,1 Iris-setosa 6,1 4,9 3,0 1,4 0,2 Iris-setosa 6,2 4,6 3,1 1,5 0,2 Iris-setosa 6,2 4,8 3,0 1,4 0,1 Iris-setosa 6,2 4,6 3,4 1,4 0,3 Iris-setosa 6,2 5,0 3,2 1,2 0,2 Iris-setosa 6,2 4,7 3,2 1,3 0,2 Iris-setosa 6,3 4,4 2,9 1,4 0,2 Iris-setosa 6,4 4,4 3,0 1,3 0,2 Iris-setosa 6,5 4,6 3,6 1,0 0,2 Iris-setosa 6,6 4,3 3,0 1,1 0,1 Iris-setosa 6,7","title":"Menghitung Jarak Data Uji Terhadap Data Sampel"},{"location":"weighted_knn/#dirangking-untuk-k-5-teratas","text":"sepal_length sepal_width petal_length petal_width class jarak 7,7 3,8 6,7 2,2 Iris-virginica 0,4 7,6 3,0 6,6 2,1 Iris-virginica 0,9 7,2 3,6 6,1 2,5 Iris-virginica 0,9 7,2 3,2 6,0 1,8 Iris-virginica 1,0 7,7 2,8 6,7 2,0 Iris-virginica 1,1","title":"Dirangking untuk k-5 teratas"},{"location":"weighted_knn/#menghitung-berat-antar-variasi","text":"Menghitung berat antar variasi dilakukan dengan rumus 1/jarak. Jarak 1/Jarak Setosa Virginica Versicolor 0.4 2.4 0 2.4 0 0.9 1.1 0 1.1 0 0.9 1.1 0 1.1 0 1.0 1.0 0 1.0 0 1.1 0.9 0 0.9 0 Jumlah 0 5.6 0","title":"Menghitung Berat Antar Variasi"},{"location":"weighted_knn/#kesimpulan","text":"Nilai terbesar adalah = Iris-Virginica dengan Nilai 5.6","title":"Kesimpulan"}]}